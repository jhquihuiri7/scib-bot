import streamlit as st
import requests
import uuid

PROD_URL = "https://6ed753424eda.ngrok-free.app"
#PROD_URL = "https://scibot-backend.uc.r.appspot.com"
#PROD_URL = "http://127.0.0.1:8000"

def upload_pdf(files, model=None):
    """
    Sube un archivo PDF al servidor backend para su procesamiento.

    Args:
        files (list): Una lista de archivos subidos (se espera que contenga un archivo PDF).
        model (str, optional): El nombre del modelo a utilizar para el procesamiento. Por defecto es None.

    Returns:
        dict or None: Un diccionario que contiene el session_id y el resumen si tiene √©xito, de lo contrario None.
    """
    file = files[0]
    files_payload = {"pdf": (file.name, file, "application/pdf")}
    
    # Construye la URL con el par√°metro del modelo si se proporciona
    url = f"{PROD_URL}/load"
    if model:
        url += f"?model={model}"

    print(model)    
    # Env√≠a la solicitud POST al backend
    response = requests.post(url, files=files_payload)

    print(response.text)
    # Maneja la respuesta
    if response.status_code == 200:
        return response.json()
    else:
        # Analiza y muestra el mensaje de error
        error_message = parse_error_message(response.text, response.status_code)
        st.error(error_message)
        return None

def parse_error_message(error_text, status_code):
    """
    Analiza y simplifica los mensajes de error para una mejor comprensi√≥n del usuario.

    Args:
        error_text (str): El texto del mensaje de error de la respuesta HTTP.
        status_code (int): El c√≥digo de estado HTTP.

    Returns:
        str: Un mensaje de error f√°cil de usar.
    """
    try:
        import json
        error_data = json.loads(error_text)
        error_msg = error_data.get("error", "")
        
        # Maneja mensajes de error espec√≠ficos
        if "Request body too large" in error_msg:
                return "‚ö†Ô∏è El documento es demasiado grande para este modelo. Intenta con un PDF m√°s peque√±o o selecciona otro modelo"
        
        elif "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return "‚ö†Ô∏è Se ha alcanzado el l√≠mite de uso del modelo. Intenta m√°s tarde o selecciona otro modelo."
        
        elif "authentication" in error_msg.lower() or "unauthorized" in error_msg.lower():
            return "‚ö†Ô∏è Error de autenticaci√≥n. Verifica la configuraci√≥n del servicio."
        
        elif "not found" in error_msg.lower():
            return "‚ö†Ô∏è El modelo seleccionado no est√° disponible. Intenta con otro modelo."
        
        else:
            return f"‚ö†Ô∏è Error del servidor: {error_msg}"
    
    except:
        # Maneja errores basados en c√≥digos de estado
        if status_code == 400:
            return "‚ö†Ô∏è Error en la solicitud. Verifica el documento o intenta con otro modelo."
        elif status_code == 429:
            return "‚ö†Ô∏è Demasiadas solicitudes. Espera un momento antes de intentar nuevamente."
        elif status_code == 500:
            return "‚ö†Ô∏è Error interno del servidor. Intenta nuevamente en unos minutos o prueba con otro modelo."
        else:
            return f"‚ö†Ô∏è Error del servicio (c√≥digo {status_code}). Intenta nuevamente."

def ask_question(session_id, question, model=None):
    """
    Env√≠a una pregunta al backend y obtiene una respuesta.

    Args:
        session_id (str): El ID de la sesi√≥n actual.
        question (str): La pregunta del usuario.
        model (str, optional): El modelo a utilizar para la respuesta. Por defecto es None.

    Returns:
        dict or None: Un diccionario que contiene la respuesta si tiene √©xito, de lo contrario None.
    """
    headers = {"Content-Type": "application/json"}
    json_data = {"message": question}
    
    # Construye la URL con los par√°metros session_id y model
    url = f"{PROD_URL}/chat?session_id={session_id}"
    if model:
        url += f"&model={model}"
    
    # Env√≠a la solicitud POST
    response = requests.post(url, headers=headers, json=json_data)

    # Maneja la respuesta
    if response.status_code == 200:
        return response.json()  # {"answer": "..."}
    else:
        # Analiza y muestra el mensaje de error
        error_message = parse_error_message(response.text, response.status_code)
        st.error(error_message)
        return None

def create_new_session(session_number):
    """
    Crea una nueva sesi√≥n con un nombre de visualizaci√≥n numerado.

    Args:
        session_number (int): El n√∫mero a mostrar para la nueva sesi√≥n.

    Returns:
        dict: Un diccionario que representa una nueva sesi√≥n.
    """
    return {
        "session_id": None,  # El session_id se asigna despu√©s de que se carga un PDF
        "chat_history": [],
        "display_name": f"Chat {session_number}",
        "has_document": False,
        "selected_model": "moonshotai/kimi-vl-a3b-thinking:free"  # Modelo por defecto
    }

def show_welcome_page():
    """
    Muestra la p√°gina de bienvenida con informaci√≥n del proyecto.
    Esta es la primera pantalla que ve el usuario.
    """
    st.set_page_config(page_title="Sci Bot - Chat con PDF", page_icon="./favicon.png", layout="wide")
    
    # Dise√±o de la pantalla de bienvenida
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.title("Generaci√≥n Autom√°tica de Res√∫menes Cient√≠ficos con Modelos NLP G9")
        
        st.markdown("---")
        
        st.markdown("""
        Este proyecto de titulaci√≥n consiste en la generaci√≥n autom√°tica de res√∫menes cient√≠ficos 
        utilizando modelos de procesamiento de lenguaje natural (NLP). Se exploran enfoques basados 
        en Deep Learning, incluyendo MLP, Transformers como BERT, y t√©cnicas de Transfer Learning 
        y Fine-Tuning, para mejorar la capacidad de los modelos de sintetizar informaci√≥n relevante 
        y producir res√∫menes precisos.
        """)
        
        st.markdown("---")
        
        # Bot√≥n de inicio
        col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
        with col_btn2:
            if st.button("Iniciar Programa", type="primary", use_container_width=True):
                st.session_state.show_main_app = True
                st.rerun()

def main():
    """
    Funci√≥n principal para ejecutar la aplicaci√≥n Streamlit.
    Maneja el estado inicial y cambia entre la p√°gina de bienvenida y la aplicaci√≥n principal.
    """
    # Inicializa el estado de la sesi√≥n para la aplicaci√≥n principal
    if "show_main_app" not in st.session_state:
        st.session_state.show_main_app = False
    
    # Muestra la p√°gina de bienvenida o la aplicaci√≥n principal seg√∫n el estado
    if not st.session_state.show_main_app:
        show_welcome_page()
    else:
        show_main_app()

def show_main_app():
    """
    Muestra la interfaz principal de la aplicaci√≥n de chat con PDF.
    Esto incluye la gesti√≥n de sesiones, pesta√±as para diferentes chats y el dise√±o principal.
    """
    st.set_page_config(page_title="Sci Bot - Chat con PDF", page_icon="./favicon.png", layout="wide")

    # Inicializa el estado de la sesi√≥n para las sesiones de chat
    if "sessions" not in st.session_state:
        st.session_state.sessions = {"default": create_new_session(1)}
    if "active_session" not in st.session_state:
        st.session_state.active_session = "default"
    if "session_counter" not in st.session_state:
        st.session_state.session_counter = 1
    
    # Se asegura de que todas las sesiones tengan un modelo predeterminado seleccionado
    for session_key, session_data in st.session_state.sessions.items():
        if "selected_model" not in session_data:
            session_data["selected_model"] = "moonshotai/kimi-vl-a3b-thinking:free"

    # Crea pesta√±as para cada sesi√≥n
    session_keys = list(st.session_state.sessions.keys())
    tab_labels = [st.session_state.sessions[key]["display_name"] for key in session_keys]
    tab_labels.append("+ Nuevo Chat")
    
    tabs = st.tabs(tab_labels)
    
    # Maneja el bot√≥n "Nuevo Chat" en la √∫ltima pesta√±a
    if len(tabs) > len(session_keys):
        with tabs[-1]:  # La √∫ltima pesta√±a es "Nuevo Chat"
            if st.button("Crear Nuevo Chat", key="new_chat_btn"):
                st.session_state.session_counter += 1
                new_session_key = f"session_{st.session_state.session_counter}"
                st.session_state.sessions[new_session_key] = create_new_session(st.session_state.session_counter)
                st.session_state.active_session = new_session_key
                st.rerun()
            st.info("Haz clic en 'Crear Nuevo Chat' para iniciar una nueva sesi√≥n.")
    
    # Muestra el contenido de cada sesi√≥n en su pesta√±a
    for i, session_key in enumerate(session_keys):
        with tabs[i]:
            st.session_state.active_session = session_key
            display_session_content(session_key)

def display_session_content(session_key):
    """
    Muestra el contenido de una sesi√≥n de chat espec√≠fica.
    Esto incluye el cargador de archivos, el historial de chat y el formulario de entrada de preguntas.

    Args:
        session_key (str): La clave de la sesi√≥n a mostrar.
    """
    current_session = st.session_state.sessions[session_key]
    
    col1, col2 = st.columns([1, 2])

    with col1:
        st.header("üì§ Subir PDF")
        uploaded_file = st.file_uploader("Carga tu PDF", type="pdf", accept_multiple_files=False, key=f"upload_{session_key}")

        if st.button("Procesar", key=f"process_{session_key}") and uploaded_file:
            with st.spinner("Procesando..."):
                selected_model = current_session.get("selected_model", "moonshotai/kimi-vl-a3b-thinking:free")
                result = upload_pdf([uploaded_file], selected_model)
                if result:
                    current_session["session_id"] = result.get("session_id")
                    summary = result.get("summary")
                    current_session["chat_history"] = []  # Restablecer el historial de chat
                    current_session["chat_history"].append(("SciBot", summary))  # Muestra el resumen en el chat
                    current_session["has_document"] = True
                    st.success("¬°Documento procesado!")
                    st.rerun()
        
        # Bot√≥n para eliminar el chat (mantiene al menos una sesi√≥n de chat)
        if len(st.session_state.sessions) > 1 and session_key != "default":
            if st.button("üóëÔ∏è Eliminar Chat", key=f"delete_{session_key}", type="secondary"):
                # Elimina la sesi√≥n
                del st.session_state.sessions[session_key]
                # Cambia a la primera sesi√≥n disponible
                remaining_sessions = list(st.session_state.sessions.keys())
                st.session_state.active_session = remaining_sessions[0]
                st.rerun()

    with col2:
        # Encabezado y selector de modelo
        col_header, col_model = st.columns([2, 1])
        
        with col_header:
            st.header("üí¨ Chat con el documento")
        
        with col_model:
            # Men√∫ desplegable de selecci√≥n de modelo
            model_options = {
                "LLama-3.3": "meta-llama/llama-3.3-70b-instruct:free",
                "Mistral": "mistralai/mistral-nemo:free",
                "Kimi VL": "moonshotai/kimi-vl-a3b-thinking:free",
                "MT5 Small": "mt5-small",
            }
            
            # Obtiene la selecci√≥n de modelo actual
            current_model_key = "Kimi VL"  # Por defecto
            for key, value in model_options.items():
                if value == current_session.get("selected_model", "moonshotai/kimi-vl-a3b-thinking:free"):
                    current_model_key = key
                    break
            
            selected_model_key = st.selectbox(
                "Modelo Elegido:",
                options=list(model_options.keys()),
                index=list(model_options.keys()).index(current_model_key),
                key=f"model_selector_{session_key}"
            )
            
            # Actualiza el modelo si la selecci√≥n cambia
            selected_model_value = model_options[selected_model_key]
            if current_session.get("selected_model") != selected_model_value:
                current_session["selected_model"] = selected_model_value
        
        # Muestra el historial de chat
        chat_container = st.container()
        with chat_container:
            if current_session["chat_history"]:
                for speaker, msg in current_session["chat_history"]:
                    if speaker == "T√∫":
                        st.markdown(f"**üßë {speaker}:** {msg}")
                    else:
                        st.markdown(f"**ü§ñ {speaker}:** {msg}")
            elif not current_session["has_document"]:
                st.info("Primero debes subir un documento a la izquierda.")
            else:
                st.info("¬°Documento cargado! Puedes hacer preguntas sobre √©l.")

        # Formulario de entrada de pregunta
        if current_session["session_id"]:
            st.markdown("---")  
            
            with st.form(key=f"question_form_{session_key}", clear_on_submit=True):
                input_col, button_col = st.columns([5, 1])
                
                with input_col:
                    question = st.text_input(
                        "Pregunta sobre el documento:", 
                        key=f"question_{session_key}",
                        placeholder="Escribe tu pregunta aqu√≠...",
                        label_visibility="visible"
                    )
                
                with button_col:
                    st.markdown("")  
                    ask_button = st.form_submit_button("Preguntar", use_container_width=True)
                
                if ask_button and question.strip():
                    with st.spinner("Obteniendo respuesta..."):
                        selected_model = current_session.get("selected_model", "moonshotai/kimi-vl-a3b-thinking:free")
                        response = ask_question(current_session["session_id"], question, selected_model)
                        if response and "answer" in response:
                            current_session["chat_history"].append(("T√∫", question))
                            current_session["chat_history"].append(("SciBot", response["answer"]))
                            st.rerun()
                elif ask_button and not question.strip():
                    st.warning("Por favor, escribe una pregunta antes de enviar.")

if __name__ == "__main__":
    main()